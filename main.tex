% mnras_template.tex
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}  % a4paper,

% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
%\usepackage{newtxtext,newtxmath}
%\usepackage{lmodern}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
\usepackage{mathptmx}
%\usepackage{txfonts}


% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage{slashbox}

%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{savesym}  % prevent symbol conflicts
\savesymbol{sf}
%\generate{%
%  \file{breqn.sty}{\nopreamble\from{breqn.dtx}{breqn.sty}}%
%}
%\usepackage{breqn} % automatic breaking equation 
%\usepackage{fancyvrb}
%\VerbatimFootnotes
\usepackage{cprotect}  % to allow verb in caption 
\DeclareMathOperator\erfc{erfc}
\DeclareMathOperator\erf{erf}
\DeclareMathOperator\cdf{cdf}
\DeclareMathOperator\sf{sf}
\DeclareMathOperator\isf{isf}
\DeclareMathOperator\ppf{ppf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[SDSS Quasars]{SDSS Stripe 82 : quasar variability from forced photometry}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[K. Suberlak et al.]{
Krzysztof Suberlak,$^{1}$\thanks{E-mail: suberlak@uw.edu}
\v{Z}eljko Ivezi\'c, $^{1}$
Yusra AlSayyad,$^{1}$ 
\\
% List of institutions
$^{1}$Department of Astronomy, University of Washington, Seattle, WA, United States\\
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2015}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}
Many objects in the universe,from stellar to extragalactic scales, are variable on timescales less than a few hundred years.  By repeated observations we can characterize the physical properties and relevant timescales of very faint objects. However, since all  astronomical observations suffer from a detection threshold, a variable object may be undetectable for certain epochs, so that lightcurve becomes incomplete. This can be circumvented by co-addition of images, allowing to detect an object, but in this process loosing all time domain information. We restore the lightcurve performing at each epoch forced photometry performed at a precise location of a variable object known from the coadded images. A challenge inherent to forced photometry is an interpretation of faint, noise-dominated flux measurement. Hitherto a standard practice has ignored the wealth of information present at the faint end, applying a cutoff 1-2 magnitudes above the detection threshold magnitude. In order to fully utilize information present in time-domain surveys, such as Large Scale Synoptic Telescope, Palomar Transient Factory, or Sloan Digital Sky Survey, we need to address the issue of characterizing faint variable object, and how their properties are affected by our handling of the faint signal. A new methodology would allow an unbiased  study of such faint variable objects, including quasars,  RR Lyrae,  Cepheids, and a wealth of other variable sources.  With the advent of precision  time-domain astronomy  surveys it is crucial to apply the best possible faint forced photometry algorithms and thus make full use of the  data.  

\section{Data Analysis}
\label{sec:data}

\subsection{Data Overview}
We use data from all SDSS runs up to an including run 7202 (Data Release 7), including all 6 SDSS camera columns. Stripe 82 survey covered an equatorial strip of the sky, defined by declination limits of $\pm1.27\deg$, extending from R.A. $\approx$ $20^{h} (320 \deg)$ to R.A.  $\approx$ $4^{h} (55 \deg)$ (Sesar+2010). Observations conducted prior to September 2005 (part of SDSS I-II) had a more sparse sampling than SDSS-III, and the SDSS Supernova Survey, which ran between September 1st - November 30th each year between 2005-2007. 

The SDSS Stripe 82 DR7  data  was processed in two data centers : NCSA (National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, IL) and IN2P3  (Institut national de physique nucl\'eaire et de physique des particules in Paris, France). NCSA processed data from $-40 \deg \, (+320 \deg) < RA < +10 \deg $ and IN2P3 with $ +5 \deg < RA < +55 \deg$ . There is a $5 \deg$ overlap, used to confirm that the data processing pipeline in both data centers yields identical data products. The entire strip was split into smaller patches

All epochs (individual images) were background-subtracted, and then scaled from the Digital Unit counts to fluxes by comparing standard objects against the Ivezic+2007 catalog  (similar to  Jiang+2014).   

\subsection{Source Detection}
Sources were detected in the i-band coadds. Each detection in the coadded images was assigned a deepSourceId (elsewhere called objectId). Considering  a dense region with clumped stars and/or galaxies, the entire clump was considered as one parent source (with single ParentSourceId). For an object which is a parent (eg. a galaxy), ParentSourceId is null. Solitary sources which are not blended  in clumps are their own parents. The result of this procedure were $~40$ million  i-band detections down to $3 \, \sigma$.  $8$ million of those are brighter than $23^{rd}$ mag. Part of Stripe82 processed in NCSA yielded  $20978391$ detections (\verb|iCoaddAll.csv|). The part that does not overlap with IN2P3 has   $16520093$ sources (\verb|iCoaddPhotometryAll.csv|), of which  $16514187$ are brighter than $30^{mag}$  ($5906$ less) (\verb|DeepSourceNCSA_i_lt300.csv|).

\subsection{Forced Photometry}
On positions specified by the detection data AlSayyad+2015 performed forced photometry in all SDSS photometric bands, on the individual epoch images. It is different from image differences technique, where the photometry is done on a difference between a coadd and an individual epoch image.  The total number of photometric measurements (combining NCSA and IN2P3) was  ($40$  million i-band detections) x ($80$ epochs ) x ($5$ filters) = $~16$ billion measurements, including   ($8$ million i-band detections i < 23) x ($80$ epochs) x ($5$ filters ) = $3.2$ billion measurements brighter than $23^{rd}$ mag.

Since an image for each epoch is background-subtracted, and the noise level fluctuates about zero,  the flux reported in an aperture at some locations may be smaller than zero. 

This can be understood realizing that background in the  optical bands is bright. We can calculate the likelihood of false detections, i.e. the background noise being confused for a source flux, in a following way. If we assume that the measured number of background counts oscillates around the value $B_{0}$, then the measured background count $B$  is distributed as  a Gaussian of width $\sigma_{B}$ :   $B-B_{0}  ~  \mathcal{N}(0,\sigma_{B})$. The noise is Poissonian, i.e. depends on  the number of counts, and since for optical mesaurements the number of counts is large,  $\sigma_{B} = \sqrt {B}$. Therefore, on a 4kx4k  CCD, with 16 Mpix, at the $5\sigma$ level (corresponding  to  1 false detection in a million), we would expect about 16 false detections.  

For each patch the raw lightcurves contain the \verb|id|, \verb|objectId|, \verb|exposure_id|, \verb|mjd|, \verb|psfFlux|, \verb|psfFluxErr|, sorted by \verb|objectId|, measuring flux in $[ergs/ cm^{2} / sec / Hz]$ (\verb|rawDataFPSplit/bandPatchStart_PatchEnd.csv|). 

\subsection{Lightcurve Metrics}
For each object we calculate lightcurve-derived metrics. Denoting \verb|psfFlux| and \verb|psfFluxErr| as $y$ and $\sigma_{y}$, we find the number of measurements per lightcurve (N), the mean flux, the median flux (the 50th quartile), median flux error \verb|e50|, mean flux error \verb|e_mean|, $\sigma_{G}$ (based on interquartile flux range 0.7413(q75-q25)), $\chi^{2}$:
\begin{equation}
\frac{1}{N-1} \sum{\left( \frac{y-mean(y)}{\sigma_{y}} \right) ^{2}}
\end{equation}
 mean weighted by the \verb|WVar| - the inverse variance (\verb|WeightedMean|), and the standard deviation weighted by inverse variance and corrected for intrinsic scatter (\verb|WeightedStdCorr|):

\begin{equation}
\mbox{WVar} = \left( \sum{\frac{1}{\sigma_{y}^{2}}} \right) ^{-1}
\end{equation} 

\begin{equation}
\mbox{WeightedMean} = \mbox{WVar} \sum{\frac{y}{\sigma_{y}^{2}}}
\end{equation}

\begin{equation}
\mbox{WeightedStdCorr} =  \left[ \frac{ \mbox{WVar}  }{N-1} \sum{\frac{(y-\mbox{WeightedMean})^{2}}{\sigma_{y}^{2}}} \right] ^{1/2}
\end{equation}



From these metrics, we can calculate the catalog photometry  :

\begin{equation}
\verb|median_mag| = -2.5 \log_{10}\mbox{median} - 48.6
\end{equation}

There are  $5892054$ sources with catalog photometry brighter than $23$ mag  (\verb|ugrizMetrics.csv|). 

We can also calculate median photometry over all individual epochs  detections, cross-matched by extinction tables [HOW ? ]  There are $12373162$ sources with median photometry, matched with E(B-V) data (\verb|medianPhotometry.csv|) . 

For each band we calculate metrics describing the lightcurve behavior for a given band, including the Butler \& Bloom classifier, which can  for high S/N objects, where it has a good discriminating power. It's advantage over the full DRW analysis for each lightcurve is that by assuming a range of $\tau$, amplitude, expected for a DRW for a QSO, we calculate the likelihood of a given lightcurve belonging to a QSO ( \verb|i_metrics.csv|). 


\subsection{Faint Sources}
\label{sec:faint_sources}
When performing forced photometry we encounter sources that are very dim ($<2\sigma$) in a single epoch image.  Consider the distribution of the likelihood of flux measurement   $L(F|data)$. For bright sources it is a very narrow Gaussian centered on the measured $F_{S}$, with width $\sigma_{F}$ on the level of $1-2 \%  \approx 0.01-0.02$ mag. However, for faint sources that have larger measurement uncertainties, the Gaussian likelihood has much wider tails. When a tail extends below zero, it  means that there is a nonzero likelihood of a negative flux measurement, which is unphysical.

As an example of prior choice, we consider a sinusoidally varying flux.  If the flux of an object over many epochs is expected to vary in a sinusoidal fashion, i.e. $F(t) = F_{min}+ sin(t)$, the probability of a given  flux measurement is a cosine, from $F_{min}$ to $F_{max}$. Thus, if no prior measurements were taken, we can confidently say that the flux of our object is most likely  $(F_{min} + F_{max}) /  2$, i.e. at the peak of the cosine likelihood function. However, as soon as one measurement is taken of $(F_{i},e_{i})$, the probability distribution of a flux of an object at that epoch becomes a convolution of our cosine prior information with the Gaussian curve of width $e_{i}$, centered on $F_{i}$ (assuming Gaussian errors). However, without any a-priori information about the variability pattern of the considered object, the least informative Bayesian prior we can impose is a flat one : $p(F)=0 $ for $F<0$, and $1$ elsewhere. Thus the posterior probability is 

\begin{equation}
p(F|data) \propto L(F|data) p(F)
\end{equation}

Such flat prior would only affect the faint sources, by moving the posterior distribution above zero flux.  The measurement of flux for bright sources would not be affected. We chose to apply the Bayesian prior  for faint measurements where  $ \langle F_{L} \rangle  < k \sigma$, with $k=2$. This corresponds to the $2\%$ probability of $F_{L} < 0$  assuming Gaussian likelihood. 

To test our method we generate fiducial lightcurves (DRW / sinusoidal), with a uniform sampling ($N=100\div1000$). Based on the generated flux ($F_{true}$) we define the $5\sigma$ level as the robust 25-th percentile (or median) of the ensemble $F_{true}$ distribution :  $\sigma_{F} = (1/5)  F_{25 \%}$ (in reality, $\sigma$ increases for fainter observations, but this is a good approximation). 
\newline
We define $F_{obs} = F_{true} + F_{noise}$, where the Gaussian noise $F_{noise} = \sigma_{F}  \, \mathcal{N}(0,1)$ was added to each point. 
For a weak signal, defined as $F_{obs}^{i} < 2 \sigma{F}$, we consider $p(F)$  - a  Gaussian  likelihood associated with $i$-th measurement: $p_{i}(F) = \mathcal{N}(\mu=F_{obs}^{i}, \sigma=\sigma_{F})$. Each measurement $F_{obs}$ is a mean of this likelihood: $F_{obs} = \langle p_{i}(F) \rangle$. We call it $p(F)$  for short : 

\begin{equation}
p(F) = \frac{1}{\sqrt{2  \pi \sigma_{F}^{2}}} \exp{ \left(-\frac{(F-\mu)^{2}}{2\sigma_{F}^{2}}\right)}
\end{equation}

 After imposing the Bayesian uniform prior $p(F)$ becomes a truncated Gaussian (without the negative part), centered on $F_{obs}^{i}$, with a width $\sigma_F$. Thus for the truncated $p(F)$  the mean ceases to be $F_{obs}^{i}$, but it can be defined as 

\begin{equation}
F_{mean} = \frac{\int _{0} ^ {\infty}{F p(F) dF}}{\int _{0} ^ {\infty}{p(F) dF}}
\end{equation} 
remembering to normalize by  integrating over the Gaussian likelihood.
 
We also define the median as  

\begin{equation}
\int _{0} ^ {F_{median}} {p(F) dF} = \int _{F_{median}} ^ {\infty} {p(F) dF}
\end{equation} 

Finally, since for a Gaussian distribution the area contained between $\mu \pm \sigma$ is $95.5 \%$ of the total area under the curve, for the truncated Gaussian we define the  $2 \sigma$ level as  two areas  $B = 0.05 A$, or : 

\begin{equation}
\int _{F_{2 \sigma}} ^{\infty} {p(F)dF} = 0.05 * \int _{0} ^{\infty} {p(F) dF} 
\end{equation}

In those cases, normalization constant on both sides of equations cancels out. 

(see Appendix )

%\begin{figure}
%\label{fig:sim_lc}
% \includegraphics[width=\columwidth]{Flux_exp_faint.png}
%\caption{A portion of a simulated lightcurve with arbitrary time and flux units, where the simulated forced photometry flux is below the 2 sigma %%level. The simulated observed points that are below 2 sigma level,  may even drop below zero, and  are subjected to our algorithm correctiong for the unphysicality of such situation. Thus we impose the non-negative flat prior, and re-calculate the flux for each point using the truncated Gaussian likelihood, by finding either mean, median, or the two sigma upper limit of the measurement. }
%\end{figure}


\subsection{Photometric colors}

Colors $x-y$ for an object with observations over many epochs are defined as the difference in magnitudes $m_{x} - m_{y}$. To find $m_{x}$, we need to define the average brightness of an object in  a given filter. With a special treatment of faint sources, substituting ($F_{obs}$,$\sigma_F$) for each faint observation by ($<F_{exp}>$,$rms$), we analyse updated lightcurves, addressing sparse sampling (see Fig.~\ref{fig:lc_example}).  

\begin{figure}
\label{fig:lc_example}
 \includegraphics[width=\columnwidth]{Lightcurve_full_obj_217720894888346422.png}
 \cprotect\caption{A plot showing an example lightcurve for an object id 217720894888346422. Jan 1st of each year (blue),  August 1st of 2005 (orange) and August 1st of each subsequent year (red) is indicated by vertical dashed lines. Observations prior to August 1st of 2005 have sparser cadence, whereas those after that date have more frequent observations.  This is due to the SDSS-III Supernova Survey which begun  Sept 1st 2005.  All points to the left of August 1st 2005 (orange line) are averaged together.  Points to the right of August 1st 2005 are seasonally averaged. }
\end{figure}

Thus for a given object we average all sparser observations prior before SDSS-III, and calculate annual averages for all subsequent years. We calculate weighted mean and the rms as 
\begin{equation}
<F> = \frac{\sum {w_{i}F_{i}}}{\sum{w_{i}}} \\
\sigma_{<F>} = \left( \sum{w_{i}}\right) ^{-1/2} 
\end{equation}

with weights as  $w_{i} = 1 / \sigma_{i}^{2}$. We also calculate the robust  median and the median error : $\sqrt{\pi / 2} \, \sigma_{F}$  [ robust $\sigma_{G} = 0.7414 * (75\% - 25\%) $ , based on the interquartile range] . Then lightcurve for a given object is reduced to one ($F_{i}, \sigma_{i}$) point prior to March 2006, and a single point per every subsequent year, where  ($F_{i}, \sigma_{i}$) is ($mean$, $meanErr$) or ($median$, $medianErr$).


The resulting average flux is converted to magnitude, and the color is  $c = m_{x}-m_{y}$, with combined errors of band lightcurves added in quadrature

\subsection{Variability}

 
Many lightcurves display  intrinsic variability, in addition to the  error-induced noise. A lightcurve consists of a set of N measurements of the object brightness  $x_{i}$  and associated errors  $e_{i}$. In this analysis we assume that  $x_{i}$ are drawn from a Gaussian distribution  $\mathcal{N}(\mu,\sigma)$, and that errors $e_{i}$ are homoscedastic, so that  the distribution of measurements is a Gaussian.  In this framework $\mu$ describes the median brightness, which for non-variable objects is the true brightness.  Using the Bayesian approach, we find $\mu$  by maximizing  the posterior probability distribution function (pdf) of  $\mu$ given $x_{i}$ and $e_{i}$ : $p(\mu | {x_{i}},{\sigma_{i}})$.  We can proceed analoguously to find the width of the distribution, $\sigma$, which describes the  departure from the mean. 

To find $\mu$ and $\sigma$, we follow Ivezic+2014, with the two-step approach. First, we find approximate values of  $\mu_{0}$ and $\sigma_{0}$, and then we evaluate the full logarithm of the posterior pdf in the vicinity of the approximate solution. The maximum of the 2D likelihood becomes our full solution - $\sigma_{full}$ and $\mu_{full}$, as shown on Fig.~\ref{fig:sigma_example} (see Appendix B for the detailed calculation). 

For each lightcurve, we also calculate mean-based $\chi^{2}_{DOF}$ and median-based $\chi^{2}_{R}$ (the latter is more robust against any outliers in the distribution) : 

\begin{equation}
\label{eqn:chi2DOF}
\chi^{2}_{dof} = \frac{1}{N-1}\sum{\left( \frac{x_{i} - <x_{i}>} {e_{i}} \right) ^{2}}
\end{equation}

\noindent and
 
\begin{equation}
\label{eqn:chi2R}
\chi^{2}_{R} = 0.7414 (Z_{75\%} - Z_{25\%} ) 
\end{equation}

\noindent with $Z=(x_{i} - median(x_{i})) / e_{i} $. 

Initially, we evaluate $\mu_{full}$, $\sigma_{full}$, $\chi^{2}_{dof}$, and $\chi^{2}_{R}$ for the entire lightcurve. Then, only if  either $\sigma_{full}>0$ or $\chi^{2}>1 $, which hints some intrinsic variability, we also calculate $mu_{full}$, $\sigma_{full}$, and $\chi^{2}$ for the seasonally-binned portions of the lightcurve.   


\begin{figure*}
\label{fig:sigma_example}
\includegraphics[width=\textwidth]{Fig_5-8_AstroML_obj_217720894888346446_.png}
\cprotect\caption{Two-step approach to finding $\mu$ and $\sigma$ via $\mu_{0}$ and $\sigma_{0}$ for an object 217720894888346446. In this calculation we use raw psf flux, before employing the faint source treatment outlined in Section~\ref{sec:faint_sources}. On the left and middle panels,  solid lines trace marginalized posterior pdfs for $\mu$ and $\sigma$ , while dashed lines depict histogram distributions of 10,000 bootstrap resamples for $\mu_{0}$ and $\sigma_{0}$. The right panel shows the logarithm of the posterior probability density function for $\mu$ and $\sigma$.}
\end{figure*}

On Fig.~\ref{fig:sigma_example} we plot the stages of calculating $\mu$ and $\sigma$. Left and middle panels compare two methods of calculating our variability parameters. The initial approximation (dashed) is based on bootstrapped resampling of $x_i$, $e_i$ points from  the lightcurve. With bootstrapping we resample the lightcurve $M$ times, so that instead of a single sample with $N \approx 10-70 $ points  we have $M$ samples. Random resampling means that points may be chosen multiple times in a random fashion. This allows us to calculate $M$ approximate values of $\mu$,$\sigma$. Dashed lines trace the histogram for $M=1000$ resamples of our variability parameters. The approximate values are in turn used to provide bounds for the $200 x 70$ grid of $\mu$, $\sigma$ on which we evaluate the full posterior likelihood density function (right panel). This ensures that we are sampling the peak of the underlying distribution. 
%Note that $\mu_{approx}$ traces well the full solution, but $\sigma_{approx}$ has a pile-up torawds 0, due to ... 


\begin{figure*}
\label{fig:lc_example_seasonal}
 \includegraphics[width=\textwidth]{Lightcurve_full_seasonal_obj_217720894888346425.png}
 \cprotect\caption{A plot showing an outcome of seasonal averaging for an object id 217720894888346425. The left panel (red dots) shows  (mean, meanErr),  and the right panel (orange) shows (median,medianErr), instead of seasonal points (blue). Vertical dashed lines as on Fig.~\ref{fig:lc_example}}
\end{figure*}



\subsection{Extinction Correction}

Since the reported fluxes are not extinction-corrected, we use a table of E(B-V) in a direction of a given source to correct for the galactic extinction. We use the formula  $x_{corr}  = x_{obs} + A_{x} * E(B-V)$, where $x$ is  u,g,r,i,z , and $A_x$ is 5.155, 3.793, 2.751, 2.086, 1.479  for each filter respectively  [Schlegel 98, Av are for RV = 3.1, also suggested by Eddie Schlafly] 




\begin{figure}
\label{fig:coadds_ext}
 \includegraphics[width=\columnwidth]{Extendedness_coadd_data_16520093_srcs_lim.png}
 \cprotect\caption{A plot showing NCSA sources detected in coadds, removing the outliers beyond the edges of the plot. The coloring corresponds to the \verb|extendedness | parameter calculated in the pipeline based on the iPsfMag-iModelMag : red being 0 (compact), and green being 1 (extended). As iModelMag increases, the separation becomes less certain, as more distant galaxies are more compact.  }
\end{figure}
% 

\section{Results}
\label{sec:results}


\begin{figure*}
\label{fig:colors_example}
\includegraphics[width=\textwidth]{Fig_g-i_vs_i_ra_310-360hist_n_50row_ext_0.png}
\cprotect\caption{A color-magnitude plot , reproducing the results of Sesar+2010 , Fig.23 .  We show here only NCSA-processed sources, which is why certain RA ranges are omitted or have less sources. We only select sources with \verb|extendedness=0| parameter (stars).  The scale is showing the $\log_{10}$ of count. All sources have their  colors corrected for extinction. On first two panels the features of Sagittarius Stream are clearly visible. }
\end{figure*}
% 



% http://tex.stackexchange.com/questions/3173/how-to-make-a-figure-span-on-two-columns-in-a-scientific-paper 
\begin{figure*}
\label{fig:coadds_ext_hist}
 \includegraphics[width=\textwidth]{Extendedness_coadd_histograms.png}
 \caption{The histograms show the count of sources in 5 magnitude bins, corresponding to the vertical cut through Fig.~\ref{fig:coadds_ext}. It helps to verify how well can the extended and compact sources be separated based solely on the iPsfMag-iModelMag}
\end{figure*}






\section{Conclusions}
\label{sec:conclusions}



\section*{Acknowledgements}

Funding for the SDSS and SDSS-II has been provided by the Alfred P. Sloan Foundation, the Participating Institutions, the National Science Foundation, the U.S. Department of Energy, the National Aeronautics and Space Administration, the Japanese Monbukagakusho, the Max Planck Society, and the Higher Education Funding Council for England. The SDSS Web Site is http://www.sdss.org/.

The SDSS is managed by the Astrophysical Research Consortium for the Participating Institutions. The Participating Institutions are the American Museum of Natural History, Astrophysical Institute Potsdam, University of Basel, University of Cambridge, Case Western Reserve University, University of Chicago, Drexel University, Fermilab, the Institute for Advanced Study, the Japan Participation Group, Johns Hopkins University, the Joint Institute for Nuclear Astrophysics, the Kavli Institute for Particle Astrophysics and Cosmology, the Korean Scientist Group, the Chinese Academy of Sciences (LAMOST), Los Alamos National Laboratory, the Max-Planck-Institute for Astronomy (MPIA), the Max-Planck-Institute for Astrophysics (MPA), New Mexico State University, Ohio State University, University of Pittsburgh, University of Portsmouth, Princeton University, the United States Naval Observatory, and the University of Washington. 

\appendix
\section{\\ Treatment of faint sources}
\label{App:AppendixA}


Using our definition, the mean is :  


\begin{equation}
F_{mean} = \frac{\int _{0} ^ {\infty}{F p(F) dF}}{\int _{0} ^ {\infty}{p(F) dF}} = I_{0} / I_{1}
\end{equation}


We evaluate 

\begin{multline}
I_{0}= \int _{0} ^ {\infty} {\frac{F}{\sqrt{2\pi\sigma_{F}^{2}}} \exp{\left(-\frac{(F-F_{obs})^{2}}{2\sigma_{F}^{2}}\right)} }dF = \\  \frac{\sigma_{F}}{\sqrt{2 \pi}} \exp{\left(- \frac{F_{obs}^{2}}{2\sigma_{F}^{2}} \right)} + F_{obs} \sf{\left( \frac{-F_{obs}}{\sigma_{F}}\right)}
\end{multline}

and 

\begin{equation}
I_{1} = \int _{0} ^ {\infty}{ p(F) dF} = \int _{0} ^ {\infty} {\frac{\exp{\left(-\frac{(F-F_{obs})^{2}}{2\sigma_{F}^{2}}\right)} }{\sqrt{2\pi\sigma_{F}^{2}}} }dF = \sf{(-F_{obs} / \sigma_{F})}
\end{equation}

so that 
\begin{equation}
x_{mean} = \frac{\exp{(- x_{obs}^{2} / 2 )} }{\sf{(-x_{obs})}\sqrt{2 \pi}} + x_{obs} 
\end{equation}

where we scaled $F_{obs}$ by  $\sigma_F$  (i.e. $F_{mean} = x_{mean} \cdot \sigma_{F}$).

\bigskip

To find the median and the $2\sigma$ level we transform from $F$ space to $x$ space , scaling by  $\sigma_{F}$, so that $x = F / \sigma_{F}$ , and thus the likelihood $p(x) \sim \mathcal{N}(x_{obs},1)$ is :
\begin{equation}
p(x) = \frac{1}{\sqrt{2  \pi }} \exp{ \left(-\frac{(x-x_{obs})^{2}}{2}\right)}
\end{equation}

Then we transform from $x$ to $z$ space, with  a translation by $x_{obs} = F_{obs} / \sigma_{F}$ :  $z = x - x_{obs}$  , so that now  $p(z) \sim \mathcal{N}(0,1)$:
\begin{equation}
p(z) = \frac{1}{\sqrt{2  \pi }} \exp{ \left(-\frac{(z)^{2}}{2}\right)}
\end{equation}



In $z$-space, the median from 

\begin{equation}
\int_{0}^{x_{med}} {p(x)dx} = \int_{x_{med}}^{\infty} {p(x)dx}
\end{equation}
 
becomes 

\begin{equation}
\int_{x_{0}}^{z_{med}}{p(z)dz} = \int_{z_{med}}^{\infty}{p(z)dz}
\end{equation}

with  $x_{0}=-x_{obs}$

\bigskip

This expression for $z_{med}$ can be evaluated : rhs is  a survival function ($\sf$) = 1 - cumulative density function ($\cdf$) : 

\begin{equation}
\int_{z_{med}}^{\infty}{p(z)dz} = \sf(z_{med})
\end{equation}

and the lhs, assuming $z_{med} > x_{0}$, is :

\begin{multline}
\int_{x_{0}}^{z_{med}}{p(z)dz} = \int_{-\infty}^{z_{med}}{p(z)dz} - \int_{-\infty}^{x_{0}}{p(z)dz} = \cdf(z_{med}) - \cdf(x_{0})
\end{multline}

Rearranging, and using the percent point function ($\ppf$) - the inverse of the $\cdf$, we find:

\begin{equation}
z_{med} = \ppf \left( \frac{1+\cdf(x_{0})}{2} \right)
\end{equation}

and transforming back to $F$ space: 

\begin{equation}
F_{med} = F_{obs} + \sigma_{F} \, \ppf \left( \frac{1+\cdf(x_{0})}{2} \right)
\end{equation}

with $x_{0}$ and $x_{obs}$ as above.  We also normalize this expression by $\int _{0} ^ {\infty}{ p(F) dF}$ and $\sigma_{F}$:

\begin{equation}
x_{med}^{norm} = x_{obs}
\end{equation}


\bigskip

In $z$  space , the $2\sigma$ areas  $A$ and $B$ are :

$\text{A} = \sf(x_{0})$ and $\text{B} = \sf(z_{B})$, so to find  $z_{B}$ we use the  inverse survival function $\isf$ : $z_{B} = \isf(0.05 \text{A})$. Thus transforming back to $F$-space we have:

\begin{equation}
F_{2\sigma} = F_{obs} + \sigma_{F} \left(\, \isf (\, 0.05 \sf (x_{0})  \right)
\end{equation}

\section{\\ Characterizing variability}
\label{App:AppendixB}


We further characterize the variability of lightcurves by calculating $\sigma_{0}$ (the approximate value), and $\sigma_{full}$, following Ivezic+2014, chapter 5. 

$\sigma_{0}$ is found in the following way: if by ($x_{i}, e_{i}$) we denote the measurement and associated error, then the bootstrapped sampling of ($x_{i}, e_{i}$) is sampling each vector at a number of N random indices (eg. N=1000). Thus instead of $x_{i}$ which may include only N=10 measurements, we have $x_{i,boot}$ which has N=1000 random samples. Median is the 50-th percentile of any sample.  Following [Ivezic+2014], chapter 5, we use the sample median to estimate $\mu_{0} = median(x_{i,boot})$, and an interquartile range width estimator to estimate the standard deviation : $\sigma_{G} =0.7413 (X_{75\%} - X_{25\%}) $ for $X = x_{i,boot}$.
With the median error $e_{50} = median(e_{i,boot})$, we estimate $\sigma_{0}$ as : 

\begin{equation}
\sigma_{0} = ( variance_{approx} )^{1/2} = (\zeta^{2} \cdot \sigma_{G}^{2} - e_{50} ^ {2})^{1/2}
\end{equation}

where 

\begin{equation}
\zeta = \frac{median(\widetilde{\sigma}_{i})} {mean(\widetilde{\sigma}_{i})}
\end{equation}

and 

\begin{equation}
\widetilde{\sigma}_{i} =  ( \widetilde{variance} )^{1/2} = ( \sigma_{G}^{2} + e_{i}^{2} - e_{50}^{2} )^{1/2}
\end{equation}


For the marginalized $\sigma_{full}$, we calculate logarithm of the posterior probability distribution for the grid of $\mu$ and $\sigma$ values as:

\begin{equation}
\log{L} = -0.5 \sum \left( \ln(\sigma^{2}+e_{i}^{2}) + \frac{(x_{i}-\mu)^{2}}{(\sigma^{2}+e_{i}^{2})} \right)
\end{equation}

We shift the maximum value of logL by subtracting the maximum value of logL, thus calculating the likelihood : 

\begin{equation}
L = e^{\log{L} - max(\log{L})}
\end{equation}

We then marginalize over $\mu$ or $\sigma$ : 

\begin{equation}
p(\sigma) = \sum_{\mu}(L_{\sigma,\mu}) \\
p(\mu) = \sum_{\sigma}(L_{\sigma,\mu})
\end{equation}

and normalize the probability :

\begin{equation}
p_{norm}(\sigma) = \frac{p(\sigma)}{ \int {p(\sigma) \text{d}\sigma}} \\ 
p_{norm}(\mu) = \frac{p(\mu)} {\int {p(\mu) \text{d}\mu} }
\end{equation}


To characterize  lightcurve variability we first calculate for the entire lightcurve of an object  the approximate $\mu_{0}$ and $\sigma_{0}$ using bootstrapped resampling of the lightcurve.  This yields the boundaries for the more exact calculation of the full 2D log-likelihood performed on a grid of $\mu$ and $\sigma$ values. Thus the more accurate $\sigma_{full}$ and $\mu_{full}$ are found as a maximum of the 2D log-likelihood distribution (see Fig.~\ref{fig:sigma_example} ).


\section{\\ Making of ugriz metrics }
\label{App:AppendixC}

Colors can be calculated in two ways: using the median of forced photometry over all epochs (object detected in coadded i-band has photometry in all epochs:  \verb|ugrizMetrics.csv|), or the median over single-epoch detections (only when an object was above the detection threshold for a single epoch : \verb|medianPhotometry.csv|).  
The median over all detections will be biased (especially for faint sources) towards higher brightness.  On the other hand, the median over all epochs will be more representative of the true brightness of an object in a given filter.  If a median brightness is negative, we can use zero point magnitudes and in such cases median over all epochs will be an upper limit on brightness, but still less biased than median over all detections. Therefore  we choose to use median over all epochs to calculate colors (see Fig.~\ref{fig:colors_example} for an example).  

\section{\\ Zero Flux Magnitudes }
\label{App:AppendixD}

If the median flux of an object over all epochs  is negative (an outcome of forced photometry on fluctuating noise), we cannot define its magnitude in that filter.  In such situation one can revert to  using for each negative flux the zero point magnitude ($m_1$) - the magnitude for a source with a flux of 1 count per sec, different for each exposure.  The zero point magnitude for each exposure with negative flux is calculated from the  Flux of 0 magnitude source,  $F_0$,  as  $m_{1} = 2.5 \log_{10}{F_{0}}$. For that object the new median magnitude in that filter will be the upper limit. We did not use this method, since a better way is to calculate the $2-\sigma$ flux limit for each flux measurement $< 2 \sigma$ : $F_{2\sigma}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%

% The best way to enter references is to use BibTeX:

%\bibliographystyle{mnras}
%\bibliography{example} % if your bibtex file is called example.bib

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}

% End of mnras_template.tex
